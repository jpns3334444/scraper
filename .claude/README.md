# Tokyo Real Estate Analysis System

A complete real estate analysis pipeline combining web scraping and AI-powered investment analysis for Tokyo properties.

## ğŸ—ï¸ System Architecture

This repository contains **two complementary systems** that work together:

### 1. Data Collection System (Scraper)
- **Location**: `scraper/` and `ai-infra/`
- **Purpose**: Web scraper that collects daily property data
- **Infrastructure**: EC2-based with CloudFormation deployment
- **Output**: CSV files and property images to S3

### 2. AI Analysis System  
- **Location**: `lambda/`, `ai-infra/`, `stepfunctions/`
- **Purpose**: AI-powered analysis using GPT-4.1 vision
- **Infrastructure**: Serverless (Lambda + Step Functions)
- **Output**: Investment reports via Slack and email

## ğŸ“Š Complete Data Flow

```mermaid
graph TB
    subgraph "Data Collection (EC2)"
        WEB[Real Estate Websites] --> SCRAPER[Web Scraper]
        SCRAPER --> S3[(S3 Storage)]
    end
    
    subgraph "AI Analysis (Serverless)"
        S3 --> ETL[ETL Lambda]
        ETL --> PROMPT[Prompt Builder]
        PROMPT --> LLM[LLM Batch]
        LLM --> REPORT[Report Sender]
    end
    
    subgraph "Outputs"
        REPORT --> EMAIL[Email]
    end
    
    subgraph "Schedule"
        CRON1[Daily: Scraper] --> SCRAPER
        CRON2[Daily: AI Analysis] --> ETL
    end
```

## ğŸš€ Quick Start

### Deploy Data Collection System

```bash
# 1. Deploy scraper infrastructure
cd cf-templates
aws cloudformation create-stack \
  --stack-name scraper-infra \
  --template-body file://infra-stack.yaml \
  --capabilities CAPABILITY_IAM

# 2. Deploy compute
aws cloudformation create-stack \
  --stack-name scraper-compute \
  --template-body file://compute-stack.yaml \
  --parameters ParameterKey=KeyName,ParameterValue=your-key-name
```

### Deploy AI Analysis System

```bash
# Deploy AI analysis pipeline
cd infra
./deploy.sh -e dev -b your-sam-bucket \
  --openai-key sk-your-openai-key \
  --email-from from@yourdomain.com \
  --email-to to@yourdomain.com
```

## ğŸ“ Repository Structure

```
â”œâ”€â”€ scraper/                    # ğŸ•·ï¸ Data Collection System
â”‚   â”œâ”€â”€ scrape.py              # Main scraper script
â”‚   â”œâ”€â”€ deploy-*.sh            # Deployment scripts
â”‚   â””â”€â”€ README.md              # Scraper documentation
â”œâ”€â”€ cf-templates/              # â˜ï¸ Scraper Infrastructure  
â”‚   â”œâ”€â”€ infra-stack.yaml       # VPC, security, IAM
â”‚   â”œâ”€â”€ compute-stack.yaml     # EC2 instance
â”‚   â””â”€â”€ README.md              # Infrastructure docs
â”œâ”€â”€ lambda/                    # ğŸ¤– AI Analysis System
â”‚   â”œâ”€â”€ etl/                   # CSV processing
â”‚   â”œâ”€â”€ prompt_builder/        # GPT-4.1 vision prompts
â”‚   â”œâ”€â”€ llm_batch/            # OpenAI Batch API
â”‚   â””â”€â”€ report_sender/         # Report generation
â”œâ”€â”€ infra/                     # â˜ï¸ AI Infrastructure
â”‚   â”œâ”€â”€ ai-stack.yaml          # Serverless infrastructure
â”‚   â””â”€â”€ deploy.sh              # AI deployment script
â”œâ”€â”€ stepfunctions/             # ğŸ”„ Workflow Orchestration
â”‚   â””â”€â”€ state_machine.asl.json # Step Functions definition
â”œâ”€â”€ tests/                     # ğŸ§ª Test Suite
â”œâ”€â”€ docs/                      # ğŸ“š Documentation
â””â”€â”€ .github/workflows/         # ğŸ”„ CI/CD Pipeline
```

## ğŸ’° Cost Analysis

### Daily Operating Costs
| Component | Cost (USD) | Cost (JPY) | Purpose |
|-----------|------------|------------|---------|
| EC2 Scraper (t3.small) | $0.60 | Â¥90 | Data collection |
| OpenAI API (Batch) | $1.17 | Â¥175 | AI analysis |
| AWS Serverless | $0.02 | Â¥3 | Processing |
| **Total Daily** | **$1.79** | **Â¥268** | Complete pipeline |

## ğŸ¯ Key Features

### Data Collection
- âœ… **Daily Automated Scraping**: Runs on EC2 infrastructure
- âœ… **Stealth Capabilities**: Browser emulation and session management
- âœ… **Image Download**: Property photos with metadata
- âœ… **Data Validation**: Clean CSV output with standardized schema

### AI Analysis  
- âœ… **GPT-4.1 Vision**: Analyzes interior property photos
- âœ… **Investment Scoring**: Ranks properties by investment potential
- âœ… **Risk Detection**: Identifies structural issues in photos
- âœ… **Cost Optimized**: OpenAI Batch API for 50% discount
- âœ… **Email Reports**: Automated email delivery of investment analysis

## ğŸ”§ Development

### Prerequisites
- AWS CLI configured
- AWS SAM CLI (for AI system)
- Docker
- Python 3.12+
- OpenAI API key

### Local Development
```bash
# Install dependencies
make install

# Run tests
make test

# Deploy to development
make deploy-dev BUCKET=... OPENAI_KEY=... SLACK_WEBHOOK=... EMAIL_FROM=... EMAIL_TO=...
```

## ğŸ“‹ Data Schema

### Scraper Output (CSV)
```csv
id,headline,price_yen,area_m2,year_built,walk_mins_station,ward,photo_filenames
listing1,"Spacious apartment",25000000,65.5,2010,8,Shibuya,"living.jpg|bedroom.jpg"
```

### AI Analysis Output  
```json
{
  "top_picks": [
    {
      "id": "listing1",
      "score": 85,
      "why": "Excellent price per mÂ² in prime location",
      "red_flags": ["Minor wear on floors visible"]
    }
  ],
  "runners_up": [...],
  "market_notes": "Strong demand in central Tokyo"
}
```

## ğŸ”„ Daily Workflow

1. **02:00 JST**: EC2 scraper collects new listings â†’ S3
2. **03:00 JST**: AI analysis processes S3 data â†’ Reports  
3. **03:05 JST**: Reports delivered via Slack + email

## ğŸ“š Documentation

- [System Architecture](docs/architecture.md) - Detailed technical design
- [Cost Analysis](docs/cost.md) - Comprehensive cost breakdown
- [Operations Runbook](docs/runbook.md) - Troubleshooting and maintenance
- [Scraper Documentation](scraper/README.md) - Data collection system
- [Infrastructure Documentation](cf-templates/README.md) - CloudFormation setup

## ğŸ”’ Security

- SSH keys for EC2 access
- OpenAI API keys in SSM Parameter Store
- IAM roles with least-privilege permissions
- VPC isolation for scraper infrastructure

## ğŸ“ Support

For issues with:
- **Data Collection**: Check EC2 instance logs and scraper documentation
- **AI Analysis**: Review CloudWatch logs and Step Functions execution
- **Deployment**: See individual README files in each system directory

---

**Complete Pipeline**: Web Scraping â†’ S3 Storage â†’ AI Analysis â†’ Investment Reports  
**Daily Target Cost**: ~Â¥268 for full analysis of 100+ properties