AWSTemplateFormatVersion: "2010-09-09"
Description: "Headless Selenium Scraper Infrastructure with S3 and Logging"

Parameters:
  KeyName:
    Type: String
    Description: Name of an existing EC2 KeyPair
  MyIPv4:
    Type: String
    Description: Your IPv4 address in CIDR (e.g. 203.0.113.1/32)
  MyIPv6:
    Type: String
    Description: Your IPv6 address in CIDR (e.g. 2400:abcd::1234/128)
  OutputBucket:
    Type: String
    Description: S3 bucket to upload scraper results to
  CreateBucket:
    Type: String
    Default: "true"
    AllowedValues: ["true", "false"]
    Description: Whether to create the S3 bucket or assume it already exists
  NotificationEmail:
    Type: String
    Default: ""
    Description: Email address for scraper notifications (optional)
  NotificationEnabled:
    Type: String
    Default: "true"
    AllowedValues: ["true", "false"]
    Description: Whether to enable email notifications

Conditions:
  ShouldCreateBucket: !Equals [!Ref CreateBucket, "true"]
  HasNotificationEmail: !Not [!Equals [!Ref NotificationEmail, ""]]
  ShouldEnableNotifications: !Equals [!Ref NotificationEnabled, "true"]

Resources:

  ScraperNotificationTopic:
    Type: AWS::SNS::Topic
    Condition: ShouldEnableNotifications
    Properties:
      TopicName: scraper-notifications
      DisplayName: Scraper Job Notifications

  ScraperNotificationSubscription:
    Type: AWS::SNS::Subscription
    Condition: HasNotificationEmail
    Properties:
      Protocol: email
      TopicArn: !Ref ScraperNotificationTopic
      Endpoint: !Ref NotificationEmail

  ScraperBucket:
    Type: AWS::S3::Bucket
    Condition: ShouldCreateBucket
    Properties:
      BucketName: !Ref OutputBucket
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true

  ScraperInstanceProfile:
    Type: AWS::IAM::InstanceProfile
    Properties:
      Roles: [!Ref ScraperRole]

  ScraperRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service: ec2.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore
      Policies:
        - PolicyName: AllowS3Upload
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - s3:PutObject
                Resource:
                  - !Sub arn:aws:s3:::${OutputBucket}/scraper-output/*
        - PolicyName: AllowS3ReadFromSource  # 🔥 ADDED THIS
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - s3:GetObject
                Resource:
                  - !Sub arn:aws:s3:::${OutputBucket}/scraper-source/*
        - PolicyName: AllowCloudWatchLogs
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - logs:CreateLogGroup
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Resource: "*"

  EventBridgeSSMExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service: scheduler.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: AllowSSMExecution
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - ssm:SendCommand
                Resource: "*"

  ScraperSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Allow SSH only from your IPs
      SecurityGroupIngress:
        - IpProtocol: tcp
          FromPort: 22
          ToPort: 22
          CidrIp: !Ref MyIPv4
        - IpProtocol: tcp
          FromPort: 22
          ToPort: 22
          CidrIpv6: !Ref MyIPv6

  ScraperInstance:
    Type: AWS::EC2::Instance
    Properties:
      InstanceType: t3.small
      KeyName: !Ref KeyName
      ImageId: ami-09013b9396188007c # Ubuntu 22.04 LTS
      IamInstanceProfile: !Ref ScraperInstanceProfile
      SecurityGroupIds:
        - !Ref ScraperSecurityGroup
      Tags:
        - Key: Name
          Value: MarketScraper
      UserData:
        Fn::Base64:
          Fn::Join:
            - ""
            - - "#!/bin/bash\n"
              - "set -e\n"
              - "exec > >(tee -a /var/log/user-data.log) 2>&1\n"
              - "echo \"🚀 Starting EC2 initialization at $(date)\" \n"
              - "apt-get update -y\n"
              - "apt-get install -y python3-pip unzip xvfb wget curl gnupg software-properties-common -y\n"
              - "wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | apt-key add -\n"
              - "echo 'deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main' > /etc/apt/sources.list.d/google-chrome.list\n"
              - "apt-get update -y\n"
              - "apt-get install -y google-chrome-stable\n"
              - "CHROME_VERSION=$(google-chrome-stable --version | cut -d' ' -f3 | cut -d'.' -f1)\n"
              - "CHROMEDRIVER_VERSION=$(curl -s https://chromedriver.storage.googleapis.com/LATEST_RELEASE_${CHROME_VERSION})\n"
              - "wget -N https://chromedriver.storage.googleapis.com/${CHROMEDRIVER_VERSION}/chromedriver_linux64.zip\n"
              - "unzip chromedriver_linux64.zip\n"
              - "mv chromedriver /usr/local/bin/\n"
              - "chmod +x /usr/local/bin/chromedriver\n"
              - "pip3 install pandas undetected-chromedriver selenium boto3 beautifulsoup4 requests\n"
              - "echo 'export OUTPUT_BUCKET=\""
              - !Ref OutputBucket
              - "\"' >> /etc/environment\n"
              - "echo 'export OUTPUT_BUCKET=\""
              - !Ref OutputBucket
              - "\"' >> /home/ubuntu/.bashrc\n"
              - "mkdir -p /var/log/scraper/\n"
              - "touch /var/log/scraper/run.log\n"
              - "chown ubuntu:ubuntu /var/log/scraper/run.log\n"
              - "wget https://s3.amazonaws.com/amazoncloudwatch-agent/ubuntu/amd64/latest/amazon-cloudwatch-agent.deb\n"
              - "dpkg -i amazon-cloudwatch-agent.deb\n"
              - "mkdir -p /opt/aws/amazon-cloudwatch-agent/etc/\n"
              - "cat << 'EOF' > /opt/aws/amazon-cloudwatch-agent/etc/config.json\n"
              - "{\n"
              - "  \"logs\": {\n"
              - "    \"logs_collected\": {\n"
              - "      \"files\": {\n"
              - "        \"collect_list\": [\n"
              - "          {\n"
              - "            \"file_path\": \"/var/log/scraper/run.log\",\n"
              - "            \"log_group_name\": \"scraper-logs\",\n"
              - "            \"log_stream_name\": \"{instance_id}\",\n"
              - "            \"timezone\": \"UTC\"\n"
              - "          }\n"
              - "        ]\n"
              - "      }\n"
              - "    }\n"
              - "  }\n"
              - "}\n"
              - "EOF\n"
              - "sleep 10\n"
              - "OUTPUT_BUCKET_NAME=\""
              - !Ref OutputBucket
              - "\"\n"
              - "echo \"Using S3 bucket: $OUTPUT_BUCKET_NAME\" >> /var/log/scraper/run.log\n"
              - "for i in {1..5}; do\n"
              - "  echo \"Attempt $i: Downloading scrape.py from S3...\" >> /var/log/scraper/run.log\n"
              - "  if aws s3 cp s3://$OUTPUT_BUCKET_NAME/scraper-source/scrape.py /home/ubuntu/scrape.py; then\n"
              - "    echo \"✅ Successfully downloaded scrape.py on attempt $i\" >> /var/log/scraper/run.log\n"
              - "    break\n"
              - "  else\n"
              - "    echo \"❌ Failed to download scrape.py on attempt $i\" >> /var/log/scraper/run.log\n"
              - "    if [ $i -eq 5 ]; then\n"
              - "      echo \"❌ All 5 download attempts failed\" >> /var/log/scraper/run.log\n"
              - "    fi\n"
              - "    sleep 5\n"
              - "  fi\n"
              - "done\n"
              - "if [ -f /home/ubuntu/scrape.py ]; then\n"
              - "  chmod +x /home/ubuntu/scrape.py\n"
              - "  chown ubuntu:ubuntu /home/ubuntu/scrape.py\n"
              - "  echo \"✅ scrape.py successfully downloaded and ready.\" >> /var/log/scraper/run.log\n"
              - "  echo \"File size: $(ls -lh /home/ubuntu/scrape.py | awk '{print $5}')\" >> /var/log/scraper/run.log\n"
              - "  # Test Python dependencies\n"
              - "  echo \"Testing Python dependencies...\" >> /var/log/scraper/run.log\n"
              - "  if python3 -c 'import pandas, requests, boto3, bs4; print(\"All dependencies OK\")' >> /var/log/scraper/run.log 2>&1; then\n"
              - "    echo \"✅ All Python dependencies verified\" >> /var/log/scraper/run.log\n"
              - "  else\n"
              - "    echo \"❌ Python dependency check failed\" >> /var/log/scraper/run.log\n"
              - "  fi\n"
              - "else\n"
              - "  echo \"❌ Failed to download scrape.py from S3 after all attempts\" >> /var/log/scraper/run.log\n"
              - "  echo \"Available S3 objects in scraper-source/:\" >> /var/log/scraper/run.log\n"
              - "  aws s3 ls s3://$OUTPUT_BUCKET_NAME/scraper-source/ >> /var/log/scraper/run.log 2>&1\n"
              - "fi\n"
              - "/opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -c file:/opt/aws/amazon-cloudwatch-agent/etc/config.json -s\n"
              - "systemctl enable amazon-ssm-agent\n"
              - "systemctl start amazon-ssm-agent\n"
              - "echo \"✅ Scraper EC2 fully initialized at $(date)\" >> /var/log/scraper/run.log\n"
              - "echo \"✅ Scraper EC2 fully initialized at $(date)\" \n"
              - "# Final status check\n"
              - "echo \"=== INITIALIZATION SUMMARY ===\" >> /var/log/scraper/run.log\n"
              - "echo \"Instance ID: $(curl -s http://169.254.169.254/latest/meta-data/instance-id)\" >> /var/log/scraper/run.log\n"
              - "echo \"Available disk space: $(df -h / | tail -1 | awk '{print $4}')\" >> /var/log/scraper/run.log\n"
              - "echo \"Python version: $(python3 --version)\" >> /var/log/scraper/run.log\n"
              - "echo \"AWS CLI version: $(aws --version)\" >> /var/log/scraper/run.log\n"
              - "echo \"OUTPUT_BUCKET: $OUTPUT_BUCKET_NAME\" >> /var/log/scraper/run.log\n"
              - "echo \"=== END SUMMARY ===\" >> /var/log/scraper/run.log\n"



  LambdaExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: LambdaSSMSendCommand
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - ssm:SendCommand
                  - ssm:GetCommandInvocation
                  - ssm:ListCommandInvocations
                Resource: "*"
        - PolicyName: LambdaBasicExecution
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - logs:CreateLogGroup
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Resource: "*"
        - PolicyName: LambdaSNSPublish
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - sns:Publish
                Resource: !Ref ScraperNotificationTopic

  ScraperTriggerLambda:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: trigger-scraper
      Handler: index.lambda_handler
      Role: !GetAtt LambdaExecutionRole.Arn
      Runtime: python3.11
      Timeout: 900
      Environment:
        Variables:
          INSTANCE_ID: !Ref ScraperInstance
          SNS_TOPIC_ARN: !Ref ScraperNotificationTopic
          NOTIFICATION_ENABLED: !Ref NotificationEnabled
      Code:
        ZipFile: |
          import boto3
          import json
          import os
          import time
          from datetime import datetime

          def lambda_handler(event, context):
              ssm = boto3.client("ssm")
              sns = boto3.client("sns")
              instance_id = os.environ["INSTANCE_ID"]
              sns_topic_arn = os.environ.get("SNS_TOPIC_ARN")
              notification_enabled = os.environ.get("NOTIFICATION_ENABLED", "true") == "true"
              
              print(f"Starting scraper job for instance: {instance_id}")
              start_time = datetime.now()
              
              try:
                  response = ssm.send_command(
                      InstanceIds=[instance_id],
                      DocumentName="AWS-RunShellScript",
                      Parameters={
                          "commands": [
                              "python3 /home/ubuntu/scrape.py >> /var/log/scraper/run.log 2>&1"
                          ],
                          "executionTimeout": ["3600"]
                      }
                  )
                  
                  command_id = response['Command']['CommandId']
                  print(f"SSM command sent: {command_id}")
                  
                  status = "InProgress"
                  while status == "InProgress":
                      time.sleep(30)
                      invocation = ssm.get_command_invocation(
                          CommandId=command_id,
                          InstanceId=instance_id
                      )
                      status = invocation['Status']
                      print(f"Command status: {status}")
                  
                  end_time = datetime.now()
                  duration = (end_time - start_time).total_seconds()
                  
                  if notification_enabled and sns_topic_arn:
                      if status == "Success":
                          message = f"✅ Scraper job completed successfully\n\nStart Time: {start_time.strftime('%Y-%m-%d %H:%M:%S UTC')}\nEnd Time: {end_time.strftime('%Y-%m-%d %H:%M:%S UTC')}\nDuration: {duration:.1f} seconds\nCommand ID: {command_id}\n\nCheck S3 bucket for scraped data."
                          subject = "Scraper Job - Success"
                      else:
                          message = f"❌ Scraper job failed\n\nStart Time: {start_time.strftime('%Y-%m-%d %H:%M:%S UTC')}\nEnd Time: {end_time.strftime('%Y-%m-%d %H:%M:%S UTC')}\nDuration: {duration:.1f} seconds\nStatus: {status}\nCommand ID: {command_id}\n\nError Output:\n{invocation.get('StandardErrorContent', 'No error details available')}"
                          subject = "Scraper Job - Failed"
                      
                      sns.publish(
                          TopicArn=sns_topic_arn,
                          Message=message,
                          Subject=subject
                      )
                  
                  return {
                      "statusCode": 200,
                      "body": json.dumps({
                          "command_id": command_id,
                          "status": status,
                          "duration": duration
                      })
                  }
                  
              except Exception as e:
                  error_msg = f"Lambda error: {str(e)}"
                  print(error_msg)
                  
                  if notification_enabled and sns_topic_arn:
                      sns.publish(
                          TopicArn=sns_topic_arn,
                          Message=f"❌ Scraper job failed to start\\n\\nError: {error_msg}",
                          Subject="Scraper Job - Error"
                      )
                  
                  return {
                      "statusCode": 500,
                      "body": json.dumps({"error": error_msg})
                  }


  ScraperLambdaEventRule:
    Type: AWS::Events::Rule
    Properties:
      Name: trigger-scraper-rule
      ScheduleExpression: "cron(0 17 * * ? *)"  # 2 AM JST = 17:00 UTC
      State: ENABLED
      Targets:
        - Arn: !GetAtt ScraperTriggerLambda.Arn
          Id: ScraperLambdaTarget

  AllowEventBridgeToInvokeLambda:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref ScraperTriggerLambda
      Action: lambda:InvokeFunction
      Principal: events.amazonaws.com
      SourceArn: !GetAtt ScraperLambdaEventRule.Arn

Outputs:
  ScraperPublicIP:
    Description: "Public IP of EC2 instance"
    Value: !GetAtt ScraperInstance.PublicIp
