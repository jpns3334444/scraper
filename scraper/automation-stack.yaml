AWSTemplateFormatVersion: "2010-09-09"
Description: "Scraper Automation - Lambda Function and EventBridge"

Parameters:
  InfraStackName:
    Type: String
    Description: Name of the infrastructure stack to import from
    Default: "tokyo-real-estate-infra"
  NotificationEnabled:
    Type: String
    Default: "true"
    AllowedValues: ["true", "false"]
    Description: Whether to enable email notifications

Resources:
  ScraperTriggerLambda:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: tokyo-real-estate-trigger
      Handler: index.lambda_handler
      Role: 
        Fn::ImportValue: !Sub "${InfraStackName}-LambdaExecutionRoleArn"
      Runtime: python3.12
      Timeout: 900
      Environment:
        Variables:
          SNS_TOPIC_ARN: 
            Fn::ImportValue: !Sub "${InfraStackName}-ScraperNotificationTopicArn"
          NOTIFICATION_ENABLED: !Ref NotificationEnabled
          DYNAMODB_TABLE:
            Fn::ImportValue: !Sub "${InfraStackName}-SessionStateTableName"
          STEP_FUNCTIONS_ARN:
            Fn::ImportValue: !Sub "${InfraStackName}-StepFunctionsArn"
          AI_WORKFLOW_ARN:
            Fn::ImportValue: tokyo-real-estate-ai-StateMachineArn
      Code:
        ZipFile: |
          import boto3
          import json
          import os
          import time
          import random
          from datetime import datetime, timezone

          def lambda_handler(event, context):
              ssm = boto3.client("ssm")
              sns = boto3.client("sns")
              ec2 = boto3.client("ec2")
              dynamodb = boto3.resource('dynamodb')
              
              sns_topic_arn = os.environ.get("SNS_TOPIC_ARN")
              notification_enabled = os.environ.get("NOTIFICATION_ENABLED", "true") == "true"
              table_name = os.environ.get("DYNAMODB_TABLE")
              
              # Extract session parameters from event
              session_id = event.get('session_id', f"session-{int(time.time())}")
              mode = event.get('mode', 'stealth')
              single_area = event.get('single_area', 'chofu-city')
              full_mode = event.get('full_mode', False)
              max_properties_override = event.get('max_properties')
              
              # Batch mode parameters
              batch_mode = event.get('batch_mode', False)
              batch_area_size = event.get('batch_area_size', 5)
              batch_number = event.get('batch_number', 1)
              
              if batch_mode:
                  print(f"üöÄ Starting {mode} scraper - Session: {session_id} - Batch {batch_number} (size: {batch_area_size})")
              else:
                  print(f"üöÄ Starting {mode} scraper - Session: {session_id}")
              
              # Configure based on mode
              if full_mode:
                  # Full load mode overrides other settings
                  max_properties = max_properties_override if max_properties_override else 0  # No limit unless override
                  stealth_mode = False
                  areas = []  # Will discover all areas
                  mode = 'full-load'
              elif mode == 'testing':
                  max_properties = max_properties_override if max_properties_override else 5
                  stealth_mode = False
                  areas = [single_area]  # Single area for testing
              elif mode == 'stealth':
                  max_properties = max_properties_override if max_properties_override else 10000
                  stealth_mode = True
                  # Use area distribution logic for comprehensive coverage
                  areas = []  # Will be handled by scraper's internal logic
              elif mode == 'normal':
                  max_properties = max_properties_override if max_properties_override else 500
                  stealth_mode = False
                  areas = [single_area]
              else:
                  max_properties = max_properties_override if max_properties_override else 8
                  stealth_mode = True
                  areas = []
              
              entry_point = event.get('entry_point', 'default')
              
              # Add human-like delay for stealth mode
              if stealth_mode:
                  initial_delay = random.uniform(300, 900)  # 5-15 minutes
                  print(f"Adding human-like delay: {initial_delay:.1f} seconds")
                  # For demo purposes, use shorter delay in Lambda
                  demo_delay = min(initial_delay, 30)  # Max 30 seconds for demo
                  time.sleep(demo_delay)
              
              # STEP 1: Start the EC2 instance if it's stopped
              print("Checking EC2 instance status...")
              try:
                  response = ec2.describe_instances(
                      Filters=[
                          {'Name': 'tag:Name', 'Values': ['tokyo-real-estate-scraper']},
                          {'Name': 'instance-state-name', 'Values': ['stopped', 'running', 'pending']}
                      ]
                  )
                  
                  instances = []
                  for reservation in response['Reservations']:
                      instances.extend(reservation['Instances'])
                  
                  # Filter out terminated instances
                  active_instances = [inst for inst in instances if inst['State']['Name'] != 'terminated']
                  
                  print(f"Found {len(instances)} total instances, {len(active_instances)} active instances")
                  
                  if not active_instances:
                      error_msg = "No MarketScraper instance found"
                      print(f"‚ùå {error_msg}")
                      
                      if notification_enabled and sns_topic_arn:
                          sns.publish(
                              TopicArn=sns_topic_arn,
                              Message=f" {mode.title()} scraper job failed to start\n\nSession: {session_id}\nError: {error_msg}",
                              Subject=f"{mode.title()} Scraper Job - Error"
                          )
                      
                      return {
                          "statusCode": 404,
                          "body": json.dumps({"error": error_msg, "session_id": session_id, "mode": mode})
                      }
                  
                  instance = active_instances[0]
                  instance_id = instance['InstanceId']
                  current_state = instance['State']['Name']
                  
                  print(f"Instance {instance_id} is currently {current_state}")
                  
                  # Start instance if needed
                  if current_state == 'stopped':
                      print(f"Starting instance {instance_id}...")
                      ec2.start_instances(InstanceIds=[instance_id])
                      
                      # Wait for instance to be running
                      waiter = ec2.get_waiter('instance_running')
                      waiter.wait(InstanceIds=[instance_id])
                      print(f"‚úÖ Instance {instance_id} is now running")
                      
                      # Wait for instance status checks to pass
                      print("Waiting for instance status checks...")
                      waiter = ec2.get_waiter('instance_status_ok')
                      waiter.wait(InstanceIds=[instance_id])
                      print(f"‚úÖ Instance {instance_id} status checks passed")
                      
                      # Wait for SSM agent to be online
                      print("Waiting for SSM agent to come online...")
                      for i in range(30):  # Max 30 seconds
                          try:
                              response = ssm.describe_instance_information(
                                  Filters=[{'Key': 'InstanceIds', 'Values': [instance_id]}]
                              )
                              if response['InstanceInformationList']:
                                  print(f"‚úÖ SSM agent is online on {instance_id}")
                                  break
                          except:
                              pass
                          time.sleep(1)
                      else:
                          print("‚ö†Ô∏è Warning: SSM agent status unclear, proceeding anyway")
                      
                  elif current_state == 'stopping':
                      print("Instance is stopping, waiting for it to stop before starting...")
                      waiter = ec2.get_waiter('instance_stopped')
                      waiter.wait(InstanceIds=[instance_id])
                      
                      # Now start it
                      ec2.start_instances(InstanceIds=[instance_id])
                      waiter = ec2.get_waiter('instance_running')
                      waiter.wait(InstanceIds=[instance_id])
                      waiter = ec2.get_waiter('instance_status_ok')
                      waiter.wait(InstanceIds=[instance_id])
                      print(f"‚úÖ Instance {instance_id} restarted and status checks passed")
                      
                  elif current_state == 'pending':
                      print("Instance is already starting, waiting for it to be ready...")
                      waiter = ec2.get_waiter('instance_running')
                      waiter.wait(InstanceIds=[instance_id])
                      waiter = ec2.get_waiter('instance_status_ok')
                      waiter.wait(InstanceIds=[instance_id])
                      print(f"‚úÖ Instance {instance_id} finished starting and status checks passed")
                  
                  # Instance should now be running
                  print(f"‚úÖ Instance {instance_id} is ready for scraping")
                  
              except Exception as e:
                  error_msg = f"Error managing EC2 instance: {str(e)}"
                  print(f"‚ùå {error_msg}")
                  
                  if notification_enabled and sns_topic_arn:
                      sns.publish(
                          TopicArn=sns_topic_arn,
                          Message=f" {mode.title()} scraper job failed to start\n\nSession: {session_id}\nError: {error_msg}",
                          Subject=f"{mode.title()} Scraper Job - Error"
                      )
                  
                  return {
                      "statusCode": 500,
                      "body": json.dumps({"error": error_msg, "session_id": session_id, "mode": mode})
                  }
              
              # STEP 2: Run the scraper command
              print(f"üèÉ Starting {mode} scraper job - Session: {session_id}, Max Properties: {max_properties}")
              start_time = datetime.now()
              
              try:
                  # Build command with session parameters
                  areas_env = ','.join(areas) if areas else ''
                  
                  # Modified command to output directly
                  scraper_command = f"""
                  export OUTPUT_BUCKET=tokyo-real-estate-ai-data
                  export SESSION_ID="{session_id}"
                  export MAX_PROPERTIES={max_properties}
                  export STEALTH_MODE={str(stealth_mode).lower()}
                  export ENTRY_POINT="{entry_point}"
                  export MODE="{mode}"
                  export AREAS="{areas_env}"
                  export FULL_LOAD={str(full_mode).lower()}
                  export BATCH_MODE={str(batch_mode).lower()}
                  export BATCH_AREA_SIZE={batch_area_size}
                  export BATCH_NUMBER={batch_number}
                  
                  # AI Workflow ARN - will be imported from ai-scraper-dev stack
                  export AI_WORKFLOW_ARN="{os.environ.get('AI_WORKFLOW_ARN', '')}"
                  
                  # Run the scraper
                  cd /home/ubuntu
                  echo "=== SCRAPER START: $(date) ==="
                  if [ "$BATCH_MODE" = "true" ]; then
                      echo "Mode: $MODE, Session: $SESSION_ID, Max Properties: $MAX_PROPERTIES, Batch: $BATCH_NUMBER (size: $BATCH_AREA_SIZE)"
                  else
                      echo "Mode: $MODE, Session: $SESSION_ID, Max Properties: $MAX_PROPERTIES"
                  fi
                  
                  # Check if scraper exists
                  if [ ! -f scrape.py ]; then
                      echo "ERROR: scrape.py not found in /home/ubuntu/"
                      ls -la /home/ubuntu/
                      exit 1
                  fi
                  
                  # Run scraper and redirect output to log file for CloudWatch
                  python3 scrape.py 2>&1 | tee -a /var/log/scraper/run.log
                  SCRAPER_EXIT_CODE=$?
                  
                  echo "=== SCRAPER END: $(date), Exit code: $SCRAPER_EXIT_CODE ==="
                  
                  # Save to log for debugging
                  mkdir -p /var/log/scraper
                  echo "Session $SESSION_ID completed with exit code: $SCRAPER_EXIT_CODE" >> /var/log/scraper/run.log
                  
                  exit $SCRAPER_EXIT_CODE
                  """
                  
                  response = ssm.send_command(
                      InstanceIds=[instance_id],
                      DocumentName="AWS-RunShellScript",
                      Parameters={
                          "commands": [scraper_command.strip()],
                          "executionTimeout": ["3600"]
                      }
                  )
                  
                  command_id = response['Command']['CommandId']
                  print(f"üìã SSM command sent: {command_id}")
                  print(f"‚ÑπÔ∏è Scraper logs are being sent directly to CloudWatch Logs group: scraper-logs")
                  
                  # Wait for command completion without streaming output
                  status = "InProgress"
                  wait_count = 0
                  max_waits = 120  # 60 minutes max (30s intervals)
                  
                  while status == "InProgress" and wait_count < max_waits:
                      time.sleep(30)  # Check every 30 seconds
                      wait_count += 1
                      
                      try:
                          invocation = ssm.get_command_invocation(
                              CommandId=command_id,
                              InstanceId=instance_id
                          )
                          status = invocation['Status']
                          
                          # Only print status updates
                          if wait_count % 4 == 0:  # Every 2 minutes
                              print(f"‚è≥ Scraper still running... ({wait_count * 30 / 60:.0f} minutes elapsed)")
                          
                      except ssm.exceptions.InvocationDoesNotExist:
                          # Command might still be initializing
                          if wait_count <= 3:
                              print("‚è≥ Waiting for command to start...")
                          continue
                  
                  end_time = datetime.now()
                  duration = (end_time - start_time).total_seconds()
                  
                  # Get final output for summary
                  if status in ['Success', 'Failed']:
                      final_invocation = ssm.get_command_invocation(
                          CommandId=command_id,
                          InstanceId=instance_id
                      )
                      output = final_invocation.get('StandardOutputContent', '')
                      error_output = final_invocation.get('StandardErrorContent', '')
                      
                      # Extract properties count from output
                      properties_scraped = 0
                      import re
                      if output:
                          match = re.search(r'(\d+) successful', output)
                          if match:
                              properties_scraped = int(match.group(1))
                  else:
                      properties_scraped = 0
                  
                  # Final status
                  print(f"\n{'‚úÖ' if status == 'Success' else '‚ùå'} Scraper {status}")
                  print(f"‚è±Ô∏è Duration: {duration:.1f} seconds")
                  print(f"üìä Properties scraped: {properties_scraped}")
                  print(f"üìã Command ID: {command_id}")
                  print(f"üîç View logs at: CloudWatch Logs > scraper-logs > {instance_id}")
                  
                  # Shutdown logic
                  if status == "Success":
                      print(f"üõë Shutting down instance {instance_id}...")
                      try:
                          ec2.stop_instances(InstanceIds=[instance_id])
                          print(f"‚úÖ EC2 stop command sent to {instance_id}")
                          instance_shutdown = True
                      except Exception as shutdown_e:
                          print(f"‚ö†Ô∏è Failed to stop instance {instance_id}: {shutdown_e}")
                          instance_shutdown = False
                  else:
                      print(f"‚ö†Ô∏è Scraper failed with status {status}. Keeping instance {instance_id} running for debugging.")
                      instance_shutdown = False

                  if notification_enabled and sns_topic_arn:
                      if status == "Success":
                          message = f" {mode.title()} scraper session completed successfully\n\nSession ID: {session_id}\nMode: {mode}\nStart Time: {start_time.strftime('%Y-%m-%d %H:%M:%S UTC')}\nEnd Time: {end_time.strftime('%Y-%m-%d %H:%M:%S UTC')}\nDuration: {duration:.1f} seconds\nProperties Scraped: {properties_scraped}\nMax Properties: {max_properties}\nCommand ID: {command_id}\nInstance ID: {instance_id}\nInstance Shutdown: {instance_shutdown}\n\nCheck CloudWatch Logs group 'scraper-logs' for detailed output."
                          subject = f"{mode.title()} Scraper Session - Success"
                      else:
                          message = f" {mode.title()} scraper session failed\n\nSession ID: {session_id}\nMode: {mode}\nStart Time: {start_time.strftime('%Y-%m-%d %H:%M:%S UTC')}\nEnd Time: {end_time.strftime('%Y-%m-%d %H:%M:%S UTC')}\nDuration: {duration:.1f} seconds\nStatus: {status}\nCommand ID: {command_id}\nInstance ID: {instance_id}\n\nInstance kept running for debugging. Check CloudWatch Logs group 'scraper-logs' for details."
                          subject = f"{mode.title()} Scraper Session - Failed"
                      
                      sns.publish(
                          TopicArn=sns_topic_arn,
                          Message=message,
                          Subject=subject
                      )
                  
                  return {
                      "statusCode": 200,
                      "body": json.dumps({
                          "command_id": command_id,
                          "status": status,
                          "duration": duration,
                          "instance_id": instance_id,
                          "session_id": session_id,
                          "mode": mode,
                          "properties_scraped": properties_scraped,
                          "max_properties": max_properties,
                          "instance_shutdown": instance_shutdown,
                          "cloudwatch_logs_group": "scraper-logs",
                          "cloudwatch_logs_stream": instance_id
                      })
                  }
                  
              except Exception as e:
                  error_msg = f"Lambda error: {str(e)}"
                  print(f"‚ùå {error_msg}")
                  
                  if notification_enabled and sns_topic_arn:
                      sns.publish(
                          TopicArn=sns_topic_arn,
                          Message=f" {mode.title()} scraper session failed to start\n\nSession: {session_id}\nError: {error_msg}",
                          Subject=f"{mode.title()} Scraper Session - Error"
                      )
                  
                  return {
                      "statusCode": 500,
                      "body": json.dumps({"error": error_msg, "session_id": session_id, "mode": mode})
                  }

  # Testing session rule - DISABLED by default
  TestingSessionRule:
    Type: AWS::Events::Rule
    Properties:
      Name: testing-session-rule
      ScheduleExpression: "cron(0 17 * * ? *)"
      State: DISABLED
      Targets:
        - Arn: !GetAtt ScraperTriggerLambda.Arn
          Id: TestingSessionTarget
          Input: |
            {
              "mode": "testing",
              "session_id": "test-session",
              "single_area": "chofu-city"
            }

  # Normal session rule - DISABLED by default  
  NormalSessionRule:
    Type: AWS::Events::Rule
    Properties:
      Name: normal-session-rule
      ScheduleExpression: "cron(0 17 * * ? *)"
      State: DISABLED
      Targets:
        - Arn: !GetAtt ScraperTriggerLambda.Arn
          Id: NormalSessionTarget
          Input: |
            {
              "mode": "normal", 
              "session_id": "daily-normal",
              "single_area": "chofu-city"
            }

  # Stealth sessions (8 rules) - ENABLED by default
  MorningSession1Rule:
    Type: AWS::Events::Rule
    Properties:
      Name: stealth-scraper-morning-1
      ScheduleExpression: "cron(0 8 * * ? *)"
      State: ENABLED
      Targets:
        - Arn: !GetAtt ScraperTriggerLambda.Arn
          Id: MorningSession1Target
          Input: |
            {
              "mode": "stealth",
              "session_id": "morning-1",
              "max_properties": 10000,
              "entry_point": "list_page_1"
            }

  MorningSession2Rule:
    Type: AWS::Events::Rule
    Properties:
      Name: stealth-scraper-morning-2
      ScheduleExpression: "cron(30 9 * * ? *)"
      State: ENABLED
      Targets:
        - Arn: !GetAtt ScraperTriggerLambda.Arn
          Id: MorningSession2Target
          Input: |
            {
              "mode": "stealth",
              "session_id": "morning-2",
              "max_properties": 10000,
              "entry_point": "search_query"
            }

  AfternoonSession1Rule:
    Type: AWS::Events::Rule
    Properties:
      Name: stealth-scraper-afternoon-1
      ScheduleExpression: "cron(15 12 * * ? *)"
      State: ENABLED
      Targets:
        - Arn: !GetAtt ScraperTriggerLambda.Arn
          Id: AfternoonSession1Target
          Input: |
            {
              "mode": "stealth",
              "session_id": "afternoon-1",
              "max_properties": 10000,
              "entry_point": "list_page_2"
            }

  AfternoonSession2Rule:
    Type: AWS::Events::Rule
    Properties:
      Name: stealth-scraper-afternoon-2
      ScheduleExpression: "cron(45 14 * * ? *)"
      State: ENABLED
      Targets:
        - Arn: !GetAtt ScraperTriggerLambda.Arn
          Id: AfternoonSession2Target
          Input: |
            {
              "mode": "stealth",
              "session_id": "afternoon-2",
              "max_properties": 10000,
              "entry_point": "price_sort"
            }

  EveningSession1Rule:
    Type: AWS::Events::Rule
    Properties:
      Name: stealth-scraper-evening-1
      ScheduleExpression: "cron(20 16 * * ? *)"
      State: ENABLED
      Targets:
        - Arn: !GetAtt ScraperTriggerLambda.Arn
          Id: EveningSession1Target
          Input: |
            {
              "mode": "stealth",
              "session_id": "evening-1",
              "max_properties": 10000,
              "entry_point": "list_page_3"
            }

  EveningSession2Rule:
    Type: AWS::Events::Rule
    Properties:
      Name: stealth-scraper-evening-2
      ScheduleExpression: "cron(10 18 * * ? *)"
      State: ENABLED
      Targets:
        - Arn: !GetAtt ScraperTriggerLambda.Arn
          Id: EveningSession2Target
          Input: |
            {
              "mode": "stealth",
              "session_id": "evening-2",
              "max_properties": 10000,
              "entry_point": "default"
            }

  NightSession1Rule:
    Type: AWS::Events::Rule
    Properties:
      Name: stealth-scraper-night-1
      ScheduleExpression: "cron(35 20 * * ? *)"
      State: ENABLED
      Targets:
        - Arn: !GetAtt ScraperTriggerLambda.Arn
          Id: NightSession1Target
          Input: |
            {
              "mode": "stealth",
              "session_id": "night-1",
              "max_properties": 10000,
              "entry_point": "area_search"
            }

  NightSession2Rule:
    Type: AWS::Events::Rule
    Properties:
      Name: stealth-scraper-night-2
      ScheduleExpression: "cron(55 22 * * ? *)"
      State: ENABLED
      Targets:
        - Arn: !GetAtt ScraperTriggerLambda.Arn
          Id: NightSession2Target
          Input: |
            {
              "mode": "stealth",
              "session_id": "night-2",
              "max_properties": 10000,
              "entry_point": "list_page_4"
            }

  # Lambda permissions for all EventBridge rules
  AllowTestingSession:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref ScraperTriggerLambda
      Action: lambda:InvokeFunction
      Principal: events.amazonaws.com
      SourceArn: !GetAtt TestingSessionRule.Arn

  AllowNormalSession:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref ScraperTriggerLambda
      Action: lambda:InvokeFunction
      Principal: events.amazonaws.com
      SourceArn: !GetAtt NormalSessionRule.Arn

  AllowMorningSession1:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref ScraperTriggerLambda
      Action: lambda:InvokeFunction
      Principal: events.amazonaws.com
      SourceArn: !GetAtt MorningSession1Rule.Arn

  AllowMorningSession2:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref ScraperTriggerLambda
      Action: lambda:InvokeFunction
      Principal: events.amazonaws.com
      SourceArn: !GetAtt MorningSession2Rule.Arn

  AllowAfternoonSession1:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref ScraperTriggerLambda
      Action: lambda:InvokeFunction
      Principal: events.amazonaws.com
      SourceArn: !GetAtt AfternoonSession1Rule.Arn

  AllowAfternoonSession2:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref ScraperTriggerLambda
      Action: lambda:InvokeFunction
      Principal: events.amazonaws.com
      SourceArn: !GetAtt AfternoonSession2Rule.Arn

  AllowEveningSession1:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref ScraperTriggerLambda
      Action: lambda:InvokeFunction
      Principal: events.amazonaws.com
      SourceArn: !GetAtt EveningSession1Rule.Arn

  AllowEveningSession2:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref ScraperTriggerLambda
      Action: lambda:InvokeFunction
      Principal: events.amazonaws.com
      SourceArn: !GetAtt EveningSession2Rule.Arn

  AllowNightSession1:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref ScraperTriggerLambda
      Action: lambda:InvokeFunction
      Principal: events.amazonaws.com
      SourceArn: !GetAtt NightSession1Rule.Arn

  AllowNightSession2:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref ScraperTriggerLambda
      Action: lambda:InvokeFunction
      Principal: events.amazonaws.com
      SourceArn: !GetAtt NightSession2Rule.Arn

Outputs:
  ScraperLambdaArn:
    Description: "ARN of the scraper trigger Lambda function"
    Value: !GetAtt ScraperTriggerLambda.Arn
    Export:
      Name: !Sub "${AWS::StackName}-ScraperLambdaArn"

  EventBridgeRulesCount:
    Description: "Number of distributed EventBridge rules created"
    Value: "10"
    Export:
      Name: !Sub "${AWS::StackName}-EventBridgeRulesCount"