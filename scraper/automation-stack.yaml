AWSTemplateFormatVersion: "2010-09-09"
Description: "Scraper Automation - Lambda Function and EventBridge"

Parameters:
  InfraStackName:
    Type: String
    Description: Name of the infrastructure stack to import from
    Default: "scraper-infra-stack"
  NotificationEnabled:
    Type: String
    Default: "true"
    AllowedValues: ["true", "false"]
    Description: Whether to enable email notifications

Resources:
  ScraperTriggerLambda:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: trigger-scraper
      Handler: index.lambda_handler
      Role: 
        Fn::ImportValue: !Sub "${InfraStackName}-LambdaExecutionRoleArn"
      Runtime: python3.12
      Timeout: 900
      Environment:
        Variables:
          SNS_TOPIC_ARN: 
            Fn::ImportValue: !Sub "${InfraStackName}-ScraperNotificationTopicArn"
          NOTIFICATION_ENABLED: !Ref NotificationEnabled
          DYNAMODB_TABLE:
            Fn::ImportValue: !Sub "${InfraStackName}-SessionStateTableName"
          STEP_FUNCTIONS_ARN:
            Fn::ImportValue: !Sub "${InfraStackName}-StepFunctionsArn"
      Code:
        ZipFile: |
          import boto3
          import json
          import os
          import time
          import random
          from datetime import datetime, timezone

          def lambda_handler(event, context):
              ssm = boto3.client("ssm")
              sns = boto3.client("sns")
              ec2 = boto3.client("ec2")
              dynamodb = boto3.resource('dynamodb')
              
              sns_topic_arn = os.environ.get("SNS_TOPIC_ARN")
              notification_enabled = os.environ.get("NOTIFICATION_ENABLED", "true") == "true"
              table_name = os.environ.get("DYNAMODB_TABLE")
              
              # Extract session parameters from event
              session_id = event.get('session_id', f"session-{int(time.time())}")
              mode = event.get('mode', 'stealth')
              single_area = event.get('single_area', 'chofu-city')
              
              # Configure based on mode
              if mode == 'testing':
                  max_properties = 5
                  stealth_mode = False
                  areas = [single_area]  # Single area for testing
              elif mode == 'stealth':
                  max_properties = event.get('max_properties', 10000)
                  stealth_mode = True
                  # Use area distribution logic for comprehensive coverage
                  areas = []  # Will be handled by scraper's internal logic
              elif mode == 'normal':
                  max_properties = 500
                  stealth_mode = False
                  areas = [single_area]
              else:
                  max_properties = event.get('max_properties', 8)
                  stealth_mode = True
                  areas = []
              
              entry_point = event.get('entry_point', 'default')
              
              # Add human-like delay for stealth mode
              if stealth_mode:
                  initial_delay = random.uniform(300, 900)  # 5-15 minutes
                  print(f"Adding human-like delay: {initial_delay:.1f} seconds")
                  # For demo purposes, use shorter delay in Lambda
                  demo_delay = min(initial_delay, 30)  # Max 30 seconds for demo
                  time.sleep(demo_delay)
              
              # STEP 1: Start the EC2 instance if it's stopped
              print("Checking EC2 instance status...")
              try:
                  response = ec2.describe_instances(
                      Filters=[
                          {'Name': 'tag:Name', 'Values': ['MarketScraper']},
                          {'Name': 'instance-state-name', 'Values': ['stopped', 'stopping', 'running', 'pending']}
                      ]
                  )
                  
                  instances = []
                  for reservation in response['Reservations']:
                      instances.extend(reservation['Instances'])
                  
                  if not instances:
                      error_msg = "No MarketScraper instance found"
                      print(error_msg)
                      
                      if notification_enabled and sns_topic_arn:
                          sns.publish(
                              TopicArn=sns_topic_arn,
                              Message=f" {mode.title()} scraper job failed to start\n\nSession: {session_id}\nError: {error_msg}",
                              Subject=f"{mode.title()} Scraper Job - Error"
                          )
                      
                      return {
                          "statusCode": 404,
                          "body": json.dumps({"error": error_msg, "session_id": session_id, "mode": mode})
                      }
                  
                  instance = instances[0]
                  instance_id = instance['InstanceId']
                  current_state = instance['State']['Name']
                  
                  print(f"Instance {instance_id} is currently {current_state}")
                  
                  # Start instance if needed
                  if current_state == 'stopped':
                      print(f"Starting instance {instance_id}...")
                      ec2.start_instances(InstanceIds=[instance_id])
                      
                      # Wait for instance to be running
                      waiter = ec2.get_waiter('instance_running')
                      waiter.wait(InstanceIds=[instance_id])
                      print(f"Instance {instance_id} is now running")
                      
                      # Give it extra time to fully initialize
                      print("Waiting 90 seconds for instance to fully boot...")
                      time.sleep(90)
                      
                  elif current_state == 'stopping':
                      print("Instance is stopping, waiting for it to stop before starting...")
                      waiter = ec2.get_waiter('instance_stopped')
                      waiter.wait(InstanceIds=[instance_id])
                      
                      # Now start it
                      ec2.start_instances(InstanceIds=[instance_id])
                      waiter = ec2.get_waiter('instance_running')
                      waiter.wait(InstanceIds=[instance_id])
                      time.sleep(90)
                      
                  elif current_state == 'pending':
                      print("Instance is already starting, waiting for it to be ready...")
                      waiter = ec2.get_waiter('instance_running')
                      waiter.wait(InstanceIds=[instance_id])
                      time.sleep(60)
                  
                  # Instance should now be running
                  print(f"Instance {instance_id} is ready for scraping")
                  
              except Exception as e:
                  error_msg = f"Error managing EC2 instance: {str(e)}"
                  print(error_msg)
                  
                  if notification_enabled and sns_topic_arn:
                      sns.publish(
                          TopicArn=sns_topic_arn,
                          Message=f" {mode.title()} scraper job failed to start\n\nSession: {session_id}\nError: {error_msg}",
                          Subject=f"{mode.title()} Scraper Job - Error"
                      )
                  
                  return {
                      "statusCode": 500,
                      "body": json.dumps({"error": error_msg, "session_id": session_id, "mode": mode})
                  }
              
              # STEP 2: Run the scraper
              print(f"Starting {mode} scraper job - Session: {session_id}, Max Properties: {max_properties}")
              start_time = datetime.now()
              
              try:
                  # Build command with session parameters
                  areas_env = ','.join(areas) if areas else ''
                  
                  # Modified command to include instance shutdown after scraping
                  scraper_command = f"""
                  export OUTPUT_BUCKET=tokyo-real-estate-ai-data
                  export SESSION_ID="{session_id}"
                  export MAX_PROPERTIES={max_properties}
                  export STEALTH_MODE={str(stealth_mode).lower()}
                  export ENTRY_POINT="{entry_point}"
                  export MODE="{mode}"
                  export AREAS="{areas_env}"
                  
                  # Run the scraper
                  cd /home/ubuntu && python3 scrape.py >> /var/log/scraper/run.log 2>&1
                  SCRAPER_EXIT_CODE=$?
                  
                  # Log completion
                  echo "Scraper completed with exit code: $SCRAPER_EXIT_CODE" >> /var/log/scraper/run.log
                  
                  # No shutdown command here, Lambda will handle it
                  """
                  
                  response = ssm.send_command(
                      InstanceIds=[instance_id],
                      DocumentName="AWS-RunShellScript",
                      Parameters={
                          "commands": [scraper_command.strip()],
                          "executionTimeout": ["3600"]
                      }
                  )
                  
                  command_id = response['Command']['CommandId']
                  print(f"SSM command sent: {command_id}")
                  
                  # Monitor command execution with timeout
                  status = "InProgress"
                  wait_count = 0
                  max_waits = 120  # 60 minutes max (30s intervals)
                  
                  while status == "InProgress" and wait_count < max_waits:
                      time.sleep(30)
                      wait_count += 1
                      
                      try:
                          invocation = ssm.get_command_invocation(
                              CommandId=command_id,
                              InstanceId=instance_id
                          )
                          status = invocation['Status']
                          print(f"Command status: {status} (check {wait_count}/{max_waits})")
                      except ssm.exceptions.InvocationDoesNotExist:
                          # Command might still be initializing
                          continue
                  
                  end_time = datetime.now()
                  duration = (end_time - start_time).total_seconds()
                  
                  # Get final invocation details
                  try:
                      final_invocation = ssm.get_command_invocation(
                          CommandId=command_id,
                          InstanceId=instance_id
                      )
                      status = final_invocation['Status']
                      output = final_invocation.get('StandardOutputContent', '')
                      error_output = final_invocation.get('StandardErrorContent', '')
                  except:
                      output = ""
                      error_output = "Could not retrieve command output"
                  
                  # Extract properties scraped from output (if available)
                  properties_scraped = 0
                  try:
                      import re
                      match = re.search(r'(\d+) successful', output)
                      if match:
                          properties_scraped = int(match.group(1))
                  except:
                      pass
                  
                  # --- NEW SHUTDOWN LOGIC ---
                  if status == "Success":
                      print(f"Scraper command completed successfully. Shutting down instance {instance_id}...")
                      try:
                          ssm.send_command(
                              InstanceIds=[instance_id],
                              DocumentName="AWS-RunShellScript",
                              Parameters={
                                  "commands": ["sudo /sbin/poweroff"],
                                  "executionTimeout": ["60"]
                              }
                          )
                          print(f"Shutdown command sent to {instance_id}.")
                          instance_shutdown = True
                      except Exception as shutdown_e:
                          print(f"Failed to send shutdown command to {instance_id}: {shutdown_e}")
                          instance_shutdown = False
                  else:
                      print(f"Scraper command failed with status {status}. Keeping instance {instance_id} running for debugging.")
                      instance_shutdown = False
                  # --- END NEW SHUTDOWN LOGIC ---

                  if notification_enabled and sns_topic_arn:
                      if status == "Success":
                          message = f" {mode.title()} scraper session completed successfully\n\nSession ID: {session_id}\nMode: {mode}\nStart Time: {start_time.strftime('%Y-%m-%d %H:%M:%S UTC')}\nEnd Time: {end_time.strftime('%Y-%m-%d %H:%M:%S UTC')}\nDuration: {duration:.1f} seconds\nProperties Scraped: {properties_scraped}\nMax Properties: {max_properties}\nCommand ID: {command_id}\nInstance ID: {instance_id}\nInstance Shutdown: {instance_shutdown}\n\nCheck S3 bucket for scraped data."
                          subject = f"{mode.title()} Scraper Session - Success"
                      else:
                          message = f" {mode.title()} scraper session failed\n\nSession ID: {session_id}\nMode: {mode}\nStart Time: {start_time.strftime('%Y-%m-%d %H:%M:%S UTC')}\nEnd Time: {end_time.strftime('%Y-%m-%d %H:%M:%S UTC')}\nDuration: {duration:.1f} seconds\nStatus: {status}\nCommand ID: {command_id}\nInstance ID: {instance_id}\n\nError Output:\n{error_output}\n\nInstance kept running for debugging."
                          subject = f"{mode.title()} Scraper Session - Failed"
                      
                      sns.publish(
                          TopicArn=sns_topic_arn,
                          Message=message,
                          Subject=subject
                      )
                  
                  return {
                      "statusCode": 200,
                      "body": json.dumps({
                          "command_id": command_id,
                          "status": status,
                          "duration": duration,
                          "instance_id": instance_id,
                          "session_id": session_id,
                          "mode": mode,
                          "properties_scraped": properties_scraped,
                          "max_properties": max_properties,
                          "instance_shutdown": instance_shutdown
                      })
                  }
                  
              except Exception as e:
                  error_msg = f"Lambda error: {str(e)}"
                  print(error_msg)
                  
                  if notification_enabled and sns_topic_arn:
                      sns.publish(
                          TopicArn=sns_topic_arn,
                          Message=f" {mode.title()} scraper session failed to start\n\nSession: {session_id}\nError: {error_msg}",
                          Subject=f"{mode.title()} Scraper Session - Error"
                      )
                  
                  return {
                      "statusCode": 500,
                      "body": json.dumps({"error": error_msg, "session_id": session_id, "mode": mode})
                  }

  # Testing session rule - DISABLED by default
  TestingSessionRule:
    Type: AWS::Events::Rule
    Properties:
      Name: testing-session-rule
      ScheduleExpression: "cron(0 17 * * ? *)"
      State: DISABLED
      Targets:
        - Arn: !GetAtt ScraperTriggerLambda.Arn
          Id: TestingSessionTarget
          Input: |
            {
              "mode": "testing",
              "session_id": "test-session",
              "single_area": "chofu-city"
            }

  # Normal session rule - DISABLED by default  
  NormalSessionRule:
    Type: AWS::Events::Rule
    Properties:
      Name: normal-session-rule
      ScheduleExpression: "cron(0 17 * * ? *)"
      State: DISABLED
      Targets:
        - Arn: !GetAtt ScraperTriggerLambda.Arn
          Id: NormalSessionTarget
          Input: |
            {
              "mode": "normal", 
              "session_id": "daily-normal",
              "single_area": "chofu-city"
            }

  # Stealth sessions (8 rules) - ENABLED by default
  MorningSession1Rule:
    Type: AWS::Events::Rule
    Properties:
      Name: stealth-scraper-morning-1
      ScheduleExpression: "cron(0 8 * * ? *)"
      State: ENABLED
      Targets:
        - Arn: !GetAtt ScraperTriggerLambda.Arn
          Id: MorningSession1Target
          Input: |
            {
              "mode": "stealth",
              "session_id": "morning-1",
              "max_properties": 10000,
              "entry_point": "list_page_1"
            }

  MorningSession2Rule:
    Type: AWS::Events::Rule
    Properties:
      Name: stealth-scraper-morning-2
      ScheduleExpression: "cron(30 9 * * ? *)"
      State: ENABLED
      Targets:
        - Arn: !GetAtt ScraperTriggerLambda.Arn
          Id: MorningSession2Target
          Input: |
            {
              "mode": "stealth",
              "session_id": "morning-2",
              "max_properties": 10000,
              "entry_point": "search_query"
            }

  AfternoonSession1Rule:
    Type: AWS::Events::Rule
    Properties:
      Name: stealth-scraper-afternoon-1
      ScheduleExpression: "cron(15 12 * * ? *)"
      State: ENABLED
      Targets:
        - Arn: !GetAtt ScraperTriggerLambda.Arn
          Id: AfternoonSession1Target
          Input: |
            {
              "mode": "stealth",
              "session_id": "afternoon-1",
              "max_properties": 10000,
              "entry_point": "list_page_2"
            }

  AfternoonSession2Rule:
    Type: AWS::Events::Rule
    Properties:
      Name: stealth-scraper-afternoon-2
      ScheduleExpression: "cron(45 14 * * ? *)"
      State: ENABLED
      Targets:
        - Arn: !GetAtt ScraperTriggerLambda.Arn
          Id: AfternoonSession2Target
          Input: |
            {
              "mode": "stealth",
              "session_id": "afternoon-2",
              "max_properties": 10000,
              "entry_point": "price_sort"
            }

  EveningSession1Rule:
    Type: AWS::Events::Rule
    Properties:
      Name: stealth-scraper-evening-1
      ScheduleExpression: "cron(20 16 * * ? *)"
      State: ENABLED
      Targets:
        - Arn: !GetAtt ScraperTriggerLambda.Arn
          Id: EveningSession1Target
          Input: |
            {
              "mode": "stealth",
              "session_id": "evening-1",
              "max_properties": 10000,
              "entry_point": "list_page_3"
            }

  EveningSession2Rule:
    Type: AWS::Events::Rule
    Properties:
      Name: stealth-scraper-evening-2
      ScheduleExpression: "cron(10 18 * * ? *)"
      State: ENABLED
      Targets:
        - Arn: !GetAtt ScraperTriggerLambda.Arn
          Id: EveningSession2Target
          Input: |
            {
              "mode": "stealth",
              "session_id": "evening-2",
              "max_properties": 10000,
              "entry_point": "default"
            }

  NightSession1Rule:
    Type: AWS::Events::Rule
    Properties:
      Name: stealth-scraper-night-1
      ScheduleExpression: "cron(35 20 * * ? *)"
      State: ENABLED
      Targets:
        - Arn: !GetAtt ScraperTriggerLambda.Arn
          Id: NightSession1Target
          Input: |
            {
              "mode": "stealth",
              "session_id": "night-1",
              "max_properties": 10000,
              "entry_point": "area_search"
            }

  NightSession2Rule:
    Type: AWS::Events::Rule
    Properties:
      Name: stealth-scraper-night-2
      ScheduleExpression: "cron(55 22 * * ? *)"
      State: ENABLED
      Targets:
        - Arn: !GetAtt ScraperTriggerLambda.Arn
          Id: NightSession2Target
          Input: |
            {
              "mode": "stealth",
              "session_id": "night-2",
              "max_properties": 10000,
              "entry_point": "list_page_4"
            }

  # Lambda permissions for all EventBridge rules
  AllowTestingSession:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref ScraperTriggerLambda
      Action: lambda:InvokeFunction
      Principal: events.amazonaws.com
      SourceArn: !GetAtt TestingSessionRule.Arn

  AllowNormalSession:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref ScraperTriggerLambda
      Action: lambda:InvokeFunction
      Principal: events.amazonaws.com
      SourceArn: !GetAtt NormalSessionRule.Arn

  AllowMorningSession1:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref ScraperTriggerLambda
      Action: lambda:InvokeFunction
      Principal: events.amazonaws.com
      SourceArn: !GetAtt MorningSession1Rule.Arn

  AllowMorningSession2:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref ScraperTriggerLambda
      Action: lambda:InvokeFunction
      Principal: events.amazonaws.com
      SourceArn: !GetAtt MorningSession2Rule.Arn

  AllowAfternoonSession1:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref ScraperTriggerLambda
      Action: lambda:InvokeFunction
      Principal: events.amazonaws.com
      SourceArn: !GetAtt AfternoonSession1Rule.Arn

  AllowAfternoonSession2:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref ScraperTriggerLambda
      Action: lambda:InvokeFunction
      Principal: events.amazonaws.com
      SourceArn: !GetAtt AfternoonSession2Rule.Arn

  AllowEveningSession1:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref ScraperTriggerLambda
      Action: lambda:InvokeFunction
      Principal: events.amazonaws.com
      SourceArn: !GetAtt EveningSession1Rule.Arn

  AllowEveningSession2:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref ScraperTriggerLambda
      Action: lambda:InvokeFunction
      Principal: events.amazonaws.com
      SourceArn: !GetAtt EveningSession2Rule.Arn

  AllowNightSession1:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref ScraperTriggerLambda
      Action: lambda:InvokeFunction
      Principal: events.amazonaws.com
      SourceArn: !GetAtt NightSession1Rule.Arn

  AllowNightSession2:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref ScraperTriggerLambda
      Action: lambda:InvokeFunction
      Principal: events.amazonaws.com
      SourceArn: !GetAtt NightSession2Rule.Arn

Outputs:
  ScraperLambdaArn:
    Description: "ARN of the scraper trigger Lambda function"
    Value: !GetAtt ScraperTriggerLambda.Arn
    Export:
      Name: !Sub "${AWS::StackName}-ScraperLambdaArn"

  EventBridgeRulesCount:
    Description: "Number of distributed EventBridge rules created"
    Value: "10"
    Export:
      Name: !Sub "${AWS::StackName}-EventBridgeRulesCount"